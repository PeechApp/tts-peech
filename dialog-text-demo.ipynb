{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from models.config import PreprocessingConfig\n",
    "from models.tts.delightful_tts.delightful_tts_refined import DelightfulTTS\n",
    "from models.vocoder.univnet import UnivNet\n",
    "from training.loss import Metrics\n",
    "from training.preprocess import TacotronSTFT\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sample_rate = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the checkpoints to the `/checkpoints` folder and choose the appropriate version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints to use for demo\n",
    "checkpoint_name = \"logs_new_training_libri-360-swa_19_sec_epoch=190-step=70670\"\n",
    "checkpoint_univnet = \"epoch=23-step=11616\"\n",
    "\n",
    "\n",
    "# checkpoint_path = f\"./logs_100/{checkpoint_name}.ckpt\"\n",
    "# checkpoint_path = f\"./logs_360_energy/{checkpoint_name}.ckpt\"\n",
    "# checkpoint_path = f\"./logs/{checkpoint_name}.ckpt\"\n",
    "checkpoint_path = f\"./checkpoints/{checkpoint_name}.ckpt\"\n",
    "checkpoint_path_univnet = f\"./checkpoints/{checkpoint_univnet}.ckpt\"\n",
    "\n",
    "# Dataset url to use for demo\n",
    "# dataset_url = \"train-clean-100\"\n",
    "dataset_url = \"train-clean-100\"\n",
    "\n",
    "# text_tts = \"\"\"\n",
    "# Casablanca: “But what about us?”\n",
    "# “We’ll always have Paris.”\n",
    "# The Wizard of Oz: “Lions? And Tigers? And Bears?”\n",
    "# “Oh my!”\n",
    "# Star Wars (A New Hope): “He’s almost in range.”\n",
    "# “That’s no moon; it’s a space station.”\n",
    "# Love Story: “Jenny, I’m sorry.”\n",
    "# “Don’t. Love means never having to say you’re sorry.”\n",
    "# No Country for Old Men: “Look, I need to know what I stand to win.”\n",
    "# “Everything.”\n",
    "# Forrest Gump: “I thought I’d try out my sea legs.”\n",
    "# “But you ain’t got no legs, Lieutenant Dan.”\n",
    "# Toy Story: “Buzz, you’re flying!”\n",
    "# “This isn’t flying; this is falling with style.”\n",
    "# As the snake shook its head, a deafening shout behind Harry made both of them jump.\n",
    "# ‘DUDLEY! MR DURSLEY! COME AND LOOK AT THIS SNAKE! YOU WON’T BELIEVE WHAT IT’S DOING!’\n",
    "# \"\"\"\n",
    "\n",
    "text_tts = \"\"\"\n",
    "Casablanca: “But what about us?”\n",
    "“We’ll always have Paris.”\n",
    "The Wizard of Oz: “Lions? And Tigers? And Bears?”\n",
    "“Oh my!”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/you/anaconda3/envs/tts_framework/lib/python3.11/site-packages/lightning/pytorch/utilities/migration/utils.py:55: The loaded checkpoint was produced with Lightning v2.2.0.post0, which is newer than your current Lightning version: v2.1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: ./checkpoints/logs_new_training_libri-360-swa_19_sec_epoch=190-step=70670.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = DelightfulTTS.load_from_checkpoint(checkpoint_path, strict=False).to(device)\n",
    "model.eval()\n",
    "\n",
    "model.vocoder_module = UnivNet()\n",
    "model.vocoder_module.eval()\n",
    "\n",
    "print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
    "\n",
    "preprocess_config = PreprocessingConfig(\"english_only\")\n",
    "tacotronSTFT = TacotronSTFT(\n",
    "    filter_length=preprocess_config.stft.filter_length,\n",
    "    hop_length=preprocess_config.stft.hop_length,\n",
    "    win_length=preprocess_config.stft.win_length,\n",
    "    n_mel_channels=preprocess_config.stft.n_mel_channels,\n",
    "    sampling_rate=preprocess_config.sampling_rate,\n",
    "    mel_fmin=preprocess_config.stft.mel_fmin,\n",
    "    mel_fmax=preprocess_config.stft.mel_fmax,\n",
    "    center=False,\n",
    ")\n",
    "tacotronSTFT = tacotronSTFT.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the speakers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(mel: np.ndarray):\n",
    "    r\"\"\"Plots the mel spectrogram.\"\"\"\n",
    "    plt.figure(dpi=80, figsize=(10, 3))\n",
    "\n",
    "    img = librosa.display.specshow(mel, x_axis=\"time\", y_axis=\"mel\", sr=sample_rate)\n",
    "    plt.title(\"Spectrogram\")\n",
    "    plt.colorbar(img, format=\"%+2.0f dB\")\n",
    "\n",
    "    # Save the plot to a BytesIO object\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert the BytesIO object to a base64 string\n",
    "    img_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    plt.close()\n",
    "\n",
    "    return img_str\n",
    "\n",
    "def gen_table(text_demo_with_metrics: list):\n",
    "    # Initialize an empty string to store the HTML\n",
    "    html = \"<table border='1'>\"\n",
    "\n",
    "    html += f\"<h4>TTS: </h4> {text_tts}\"\n",
    "\n",
    "    html += \"<h4>Speakers: </h4>\"\n",
    "    html += \"\"\"<tr>\n",
    "        <th>SpeakerID</th>\n",
    "        <th>Speaker Name</th>\n",
    "        <th>Gender</th>\n",
    "        <th>Subset</th>\n",
    "        <th>Audio</th>\n",
    "        <th>InfTime</th>\n",
    "        <th>ermr</th>\n",
    "        <th>jitter</th>\n",
    "        <th>shimmer</th>\n",
    "        <th>spec</th>\n",
    "    </tr>\"\"\"\n",
    "\n",
    "    for row in text_demo_with_metrics:\n",
    "        # Speaker info\n",
    "        speaker_id = row[\"speaker_id\"]\n",
    "        speaker_name = row[\"speaker_name\"]\n",
    "        gender = row[\"gender\"]\n",
    "        subset = row[\"subset\"]\n",
    "        execution_time = row[\"execution_time\"]\n",
    "\n",
    "        # Waveforms and mel spectrogram\n",
    "        wav = row[\"wav\"]\n",
    "        mel = row[\"mel\"]\n",
    "\n",
    "        # Metrics\n",
    "        ermr = row[\"ermr\"]\n",
    "        jitter = row[\"jitter\"]\n",
    "        shimmer = row[\"shimmer\"]\n",
    "\n",
    "        # Round the metrics to 3 decimal places\n",
    "        metrics = [round(x, 3) for x in [ermr, jitter, shimmer]]\n",
    "\n",
    "        audio = Audio(wav, rate=sample_rate, autoplay=False)\n",
    "\n",
    "        # Generate the spectrogram plot\n",
    "        fig = plot_spectrogram(mel)\n",
    "\n",
    "        # Add a row to the HTML table\n",
    "        html += f\"\"\"<tr>\n",
    "            <td>{speaker_id}</td>\n",
    "            <td>{speaker_name}</td>\n",
    "            <td>{gender}</td>\n",
    "            <td>{subset}</td>\n",
    "            <td>{audio._repr_html_()}</td>\n",
    "            <td>{execution_time}</td>\n",
    "            <td>{metrics[0]}</td>\n",
    "            <td>{metrics[1]}</td>\n",
    "            <td>{metrics[2]}</td>\n",
    "            <td><img src='data:image/png;base64,{fig}' /></td>\n",
    "        </tr>\"\"\"\n",
    "\n",
    "    # Close the HTML table\n",
    "    html += \"</table>\"\n",
    "\n",
    "    return HTML(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch_table(batch: list, batch_idx: int):\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    # Get the unique cluster labels\n",
    "    df_ermr_sorted = df.sort_values(by=\"ermr\")\n",
    "    metrics = [\"ermr\", \"jitter\", \"shimmer\"]\n",
    "\n",
    "    demo_dirname = f\"logs/{dataset_url}_{checkpoint_name}\"\n",
    "    os.makedirs(demo_dirname, exist_ok=True)\n",
    "\n",
    "    # Add header\n",
    "    result = f\"\"\"\n",
    "    <h3>Chunk #{batch_idx}: </h3>\n",
    "    {df_ermr_sorted[[\"speaker_id\", *metrics]].describe().to_html()}\n",
    "    <h3>Chunk #{batch_idx} audio data: </h3>\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate and display the table for the chunk\n",
    "    # for _, row in df_ermr_sorted.iterrows():\n",
    "    result += gen_table(df_ermr_sorted.to_dict(\"records\")).data # type: ignore\n",
    "\n",
    "    # Save result as HTML\n",
    "    with open(f\"{demo_dirname}/chunk_{batch_idx}.html\", \"w\") as f:\n",
    "        f.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_tts = \"\"\"\n",
    "# Casablanca: “But what about us?”\n",
    "# “We’ll always have Paris.”\n",
    "# The Wizard of Oz: “Lions? And Tigers? And Bears?”\n",
    "# “Oh my!”\n",
    "# Star Wars (A New Hope): “He’s almost in range.”\n",
    "# “That’s no moon; it’s a space station.”\n",
    "# Love Story: “Jenny, I’m sorry.”\n",
    "# “Don’t. Love means never having to say you’re sorry.”\n",
    "# No Country for Old Men: “Look, I need to know what I stand to win.”\n",
    "# “Everything.”\n",
    "# Forrest Gump: “I thought I’d try out my sea legs.”\n",
    "# “But you ain’t got no legs, Lieutenant Dan.”\n",
    "# Toy Story: “Buzz, you’re flying!”\n",
    "# “This isn’t flying; this is falling with style.”\n",
    "# As the snake shook its head, a deafening shout behind Harry made both of them jump.\n",
    "# ‘DUDLEY! MR DURSLEY! COME AND LOOK AT THIS SNAKE! YOU WON’T BELIEVE WHAT IT’S DOING!’\n",
    "# \"\"\"\n",
    "\n",
    "text_tts = \"\"\"\n",
    "The Wizard of Oz: “Lions? And Tigers? And Bears?”\n",
    "Toy Story: “Buzz, you’re flying!”\n",
    "As the snake shook its head, a deafening shout behind Harry made both of them jump.\n",
    "‘DUDLEY! MR DURSLEY! COME AND LOOK AT THIS SNAKE! YOU WON’T BELIEVE WHAT IT’S DOING!’\n",
    "\"\"\"\n",
    "\n",
    "with open(\"training/datasets/speaker_id_mapping_libri.json\") as f:\n",
    "    id_mapping = json.load(f)\n",
    "\n",
    "batch_size = 200\n",
    "model.vocoder_module.to(device)\n",
    "\n",
    "metrics = Metrics()\n",
    "text_demo_with_metrics = []\n",
    "\n",
    "speakers_df = pd.read_csv(\n",
    "    \"./datasets_cache/LIBRITTS/LibriTTS/speakers.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"READER\", \"GENDER\", \"SUBSET\", \"NAME\"],\n",
    ")\n",
    "\n",
    "subsets = [\"train-clean-100\", \"train-clean-360\", \"train-other-500\"]\n",
    "\n",
    "speakers = speakers_df[speakers_df['SUBSET'].isin(subsets)]\n",
    "\n",
    "start_chunk = 12  # The chunk to start from\n",
    "start_idx = start_chunk * batch_size  # The index to start from\n",
    "\n",
    "for idx, speaker in enumerate(speakers.to_dict(\"records\")):\n",
    "    # Skip the speakers that have already been processed\n",
    "    if idx < start_idx:\n",
    "        continue\n",
    "\n",
    "    speaker_id = speaker[\"READER\"]\n",
    "    speaker_id = id_mapping.get(str(speaker_id))\n",
    "    speaker_name = speaker[\"NAME\"]\n",
    "    gender = speaker[\"GENDER\"]\n",
    "    subset = speaker[\"SUBSET\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        speaker_id_ = torch.tensor([int(speaker_id)], device=device)\n",
    "        start_time = time.time()\n",
    "        wav = model.forward(text_tts, speaker_id_)\n",
    "        end_time = time.time()\n",
    "        (\n",
    "            ermr,\n",
    "            jitter,\n",
    "            shimmer,\n",
    "        ) = metrics.wav_metrics(wav.unsqueeze(0))\n",
    "        execution_time = round(end_time - start_time, 2)\n",
    "\n",
    "        mel = tacotronSTFT.get_mel_from_wav(wav)\n",
    "\n",
    "        text_demo_with_metrics.append({\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"speaker_name\": speaker_name,\n",
    "            \"gender\": gender,\n",
    "            \"subset\": subset,\n",
    "            \"wav\": wav.detach().cpu().numpy(),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"mel\": mel.detach().cpu().numpy(),\n",
    "            \"ermr\": ermr,\n",
    "            \"jitter\": jitter,\n",
    "            \"shimmer\": shimmer,\n",
    "        })\n",
    "\n",
    "    if idx > 0 and idx % batch_size == 0:\n",
    "        gen_batch_table(text_demo_with_metrics, idx // batch_size)\n",
    "        text_demo_with_metrics = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gen_batch_table(text_demo_with_metrics, 999)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
