## Training code

Docs for the training code are here: [Modules](./modules/readme.md)

Here I describe only the training process, approaches, and technical details of the implementation. What is changed compared to the [original implementation dunky11/voicesmith](https://github.com/dunky11/voicesmith)

And crucial ideas behind the training.

## Major Changes

#### 1 Text preprocessing is changed.

Dunky11 created a not-so-good way for text preprocessing. Maybe his solutions are appropriate for the previous releases of [NeMo](https://github.com/NVIDIA/NeMo), but now it works stable. I can't find the described scenarios:
*Dunky11 quote*:
> Nemo's text normalizer unfortunately produces a large amount of false positives. For example it normalizes 'medic' into 'm e d i c' or 'yeah' into 'y e a h'. To reduce the amount of false positives we will do a check for unusual symbols or words inside the text and only normalize if necessary.
I checked the described cases and it works fine. Tried to find an issue, but wasn't lucky. Maybe I did it wrong.
I have the code, that wrapped the `NeMo` and added several more preprocessing features, that absolutely required, like char mapping. You can find docs here: [NormalizeText](./preprocess/normalize_text.md)

#### 2 The phonemizer process (G2P) and tokenization process are changed.

I tried to build the same tokenization as Dunky11, but failed, because of the vocab. It's not possible to reproduce the same vocab in the same order, and the vocab that I have missesed some `IPA` tokens. Change of the vocab order == lost all the progress that was made from the training steps. So, it makes no sense to build my own tokenization that lost benefits during the training process. I decided to use tokenizations from [DeepPhonemizer](https://github.com/as-ideas/DeepPhonemizer), maybe Dunky11 didn't find it, I don't understand why he's built his own solution.

Maybe because of the `[SILENCE]` token from here:

```python
for sentence in sentence_tokenizer.tokenize(text):
    symbol_ids = []
    sentence = text_normalizer(sentence)
    for word in word_tokenizer.tokenize(sentence):
        word = word.lower()
        if word.strip() == "":
            continue
        elif word in [".", "?", "!"]:
            symbol_ids.append(self.symbol2id[word])
        elif word in [",", ";"]:
            symbol_ids.append(self.symbol2id["SILENCE"])
```

#### 3 Training framework instead of tricky training spaghetti

[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) instead of wild hacks.

#### 4 Bug fixing.

During the development process, a huge amount of job has been done. For example, the `stft` code changed to the latest way of conversion from `complex` to `real`. For example:

```python
# Compute the short-time Fourier transform of the input waveform
x = torch.stft(
    x,
    n_fft=n_fft,
    hop_length=hop_length,
    win_length=win_length,
    center=False,
    return_complex=True,
    # Add window parameter to prevent the signal leak
    window=torch.ones(win_length, device=x.device),
)  # [B, F, TT, 2]

# Convert to real as the additional step
x = torch.view_as_real(x)
```

It's a tested fix, and works stable.

Minor fixes, and `TODO` that I can't fix now, but wanted to fix for the future.

The ideas behind the model and architecture are mostly the same. The training code is completely different, there is a base of dunky11 and the architecture is completely new.

## Basic Checkpoints

Legacy checkpoint is taken from [dunky11/voicesmith](https://github.com/dunky11/voicesmith)
Here is the link to [checkpoints](https://drive.google.com/drive/folders/15VQgRxGO_Z_RUNMyuJreg9O5Ckcit2vh)

These are the basic checkpoints. Mostly useless, with these checkpoints and new model blocks it's not possible to generate text-to-speech, and because of the major changes like `G2P`.

To upload the legacy checkpoints, inside the [acoustic_module](./modules/acoustic_module.md) and [vocoder_module](./modules/vocoder_module.md) I have `checkpoint_path_v1` argument. This will be removed, because of the new model states were generated by the modules.

#### NOTE FIXME: Step param!

I have a `step` parameter, it requires the future investigation. Maybe I need to add this param to the model step with `self.register_buffer`. It's required for the [FastSpeech2LossGen](./loss/fast_speech_2_loss_gen.md)
Possible issue in training.

## Lightning checkpoints

The new checkpoints, generated by `lightning`, are very easy to save and load:

```python
trainer = Trainer(
    # Save checkpoints to the `default_root_dir` directory
    default_root_dir="checkpoints/acoustic",
    limit_train_batches=2,
    max_epochs=1,
    accelerator="cuda",
)

# Training process...
result = trainer.fit(model=module, train_dataloaders=train_dataloader)

# ...

# Restore from the checkpoint
acoustic_module = AcousticModule.load_from_checkpoint(
    "./checkpoints/am_pitche_stats.ckpt",
)
vocoder_module = VocoderModule.load_from_checkpoint(
    "./checkpoints/vocoder.ckpt",
)

```

#### [More details are here: Transfer Learning](https://lightning.ai/docs/pytorch/stable/advanced/transfer_learning.html)

Inside the `AcousticModule` I have the `pitches_stat`, parameter, it's required for the audio generation.
Without the new weights that keep-in-track `pitches_stat, it's not possible to generate the waveform or mel-spectrogram.

## Early Stopping

You can stop and skip the rest of the current epoch early by overriding `on_train_batch_start()` to return `-1` when some condition is met.

### EarlyStopping Callback

The `EarlyStopping` callback can be used to monitor a metric and stop the training when no improvement is observed.

```python
from lightning.pytorch.callbacks.early_stopping import EarlyStopping

# ...
trainer = Trainer(
    # Save checkpoints to the `default_root_dir` directory
    default_root_dir="checkpoints/acoustic",
    limit_train_batches=2,
    max_epochs=1,
    accelerator="cuda",
    # Need to define the criterias
    callbacks=[EarlyStopping(monitor="val_loss", mode="min")]
)
```

#### [More details are here: EarlyStopping](https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)

## Hyperparameters from the CLI

The `ArgumentParser` is a built-in feature in Python that let's you build CLI programs. You can use it to make hyperparameters and other training settings available from the command line:

```python
from argparse import ArgumentParser

parser = ArgumentParser()

# Trainer arguments
parser.add_argument("--devices", type=int, default=2)

# Hyperparameters for the model
parser.add_argument("--layer_1_dim", type=int, default=128)

# Parse the user inputs and defaults (returns a argparse.Namespace)
args = parser.parse_args()

# Use the parsed arguments in your program
trainer = Trainer(devices=args.devices)
model = MyModel(layer_1_dim=args.layer_1_dim)
```

This allows you to call your program like so:

```bash
python trainer.py --layer_1_dim 64 --devices 1
```

#### [More details are here: Configure hyperparameters from the CLI](https://lightning.ai/docs/pytorch/stable/common/hyperparameters.html)

### Docs and examples:

#### [argparse â€” Parser for command-line options, arguments and sub-commands](https://docs.python.org/3/library/argparse.html)

#### [Command-Line Option and Argument Parsing using argparse in Python](https://www.geeksforgeeks.org/command-line-option-and-argument-parsing-using-argparse-in-python/)

#### [Build Command-Line Interfaces With Python's argparse](https://realpython.com/command-line-interfaces-python-argparse/)

